{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_Austausch ANTS\n",
      "1_Organisatorisches und Aufgabenstellung\n",
      "2_Hinweise, Leitfäden und Arbeitssicherheit\n",
      "3_Literatur\n",
      "4_Experimentelle Daten\n",
      "5_Dokumentation\n",
      "6_Schriftliche Ausarbeitung\n",
      "7_Präsentation\n",
      "WICHTIG_Read_Me_FAQ_wissenschaftliche_Arbeiten.docx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for file_name in os.listdir(\n",
    "        r\"C:\\Users\\hfu\\Desktop\\Wissenschaftliche Arbeiten Ordner\\Wissenschaftliche Arbeiten Ordner\"\n",
    "    ):\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfuhaodsst\u001b[0m (\u001b[33mapes3\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\hfu\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Mult-Adds\n",
       "=================================================================================================================================================\n",
       "ShapeNetModel_fpsknn                          [8, 3, 2048]              [8, 50, 2048]             --                        --\n",
       "├─STN: 1-1                                    [8, 6, 2048, 32]          [8, 3, 3]                 --                        --\n",
       "│    └─Sequential: 2-1                        [8, 6, 2048, 32]          [8, 64, 2048, 32]         --                        --\n",
       "│    │    └─Conv2d: 3-1                       [8, 6, 2048, 32]          [8, 64, 2048, 32]         384                       201,326,592\n",
       "│    │    └─BatchNorm2d: 3-2                  [8, 64, 2048, 32]         [8, 64, 2048, 32]         128                       1,024\n",
       "│    │    └─LeakyReLU: 3-3                    [8, 64, 2048, 32]         [8, 64, 2048, 32]         --                        --\n",
       "│    └─Sequential: 2-2                        [8, 64, 2048, 32]         [8, 128, 2048, 32]        --                        --\n",
       "│    │    └─Conv2d: 3-4                       [8, 64, 2048, 32]         [8, 128, 2048, 32]        8,192                     4,294,967,296\n",
       "│    │    └─BatchNorm2d: 3-5                  [8, 128, 2048, 32]        [8, 128, 2048, 32]        256                       2,048\n",
       "│    │    └─LeakyReLU: 3-6                    [8, 128, 2048, 32]        [8, 128, 2048, 32]        --                        --\n",
       "│    └─Sequential: 2-3                        [8, 128, 2048]            [8, 1024, 2048]           --                        --\n",
       "│    │    └─Conv1d: 3-7                       [8, 128, 2048]            [8, 1024, 2048]           131,072                   2,147,483,648\n",
       "│    │    └─BatchNorm1d: 3-8                  [8, 1024, 2048]           [8, 1024, 2048]           2,048                     16,384\n",
       "│    │    └─LeakyReLU: 3-9                    [8, 1024, 2048]           [8, 1024, 2048]           --                        --\n",
       "│    └─Sequential: 2-4                        [8, 1024]                 [8, 512]                  --                        --\n",
       "│    │    └─Linear: 3-10                      [8, 1024]                 [8, 512]                  524,288                   4,194,304\n",
       "│    │    └─BatchNorm1d: 3-11                 [8, 512]                  [8, 512]                  1,024                     8,192\n",
       "│    │    └─LeakyReLU: 3-12                   [8, 512]                  [8, 512]                  --                        --\n",
       "│    └─Dropout: 2-5                           [8, 512]                  [8, 512]                  --                        --\n",
       "│    └─Sequential: 2-6                        [8, 512]                  [8, 256]                  --                        --\n",
       "│    │    └─Linear: 3-13                      [8, 512]                  [8, 256]                  131,072                   1,048,576\n",
       "│    │    └─BatchNorm1d: 3-14                 [8, 256]                  [8, 256]                  512                       4,096\n",
       "│    │    └─LeakyReLU: 3-15                   [8, 256]                  [8, 256]                  --                        --\n",
       "│    └─Dropout: 2-7                           [8, 256]                  [8, 256]                  --                        --\n",
       "│    └─Linear: 2-8                            [8, 256]                  [8, 9]                    2,313                     18,504\n",
       "├─FeatureLearningBlock_fpsknn: 1-2            [8, 3, 2048]              [8, 128, 2048]            --                        --\n",
       "│    └─ModuleList: 2-9                        --                        --                        --                        --\n",
       "│    │    └─EdgeConv: 3-16                    [8, 3, 2048]              [8, 64, 2048]             4,736                     2,348,812,288\n",
       "│    │    └─EdgeConv: 3-17                    [8, 64, 2048]             [8, 64, 2048]             12,544                    6,442,452,992\n",
       "│    └─ModuleList: 2-18                       --                        --                        (recursive)               --\n",
       "│    │    └─Neighbor2PointAttention: 3-18     [8, 128, 2048]            [8, 128, 2048]            180,736                   19,595,792,384\n",
       "│    └─ModuleList: 2-13                       --                        --                        (recursive)               --\n",
       "│    │    └─PointNetSetAbstraction: 3-19      [8, 3, 2048]              [8, 128, 1024]            133,760                   34,728,847,360\n",
       "│    └─ModuleList: 2-18                       --                        --                        (recursive)               --\n",
       "│    │    └─Neighbor2PointAttention: 3-20     [8, 128, 1024]            [8, 128, 1024]            180,736                   9,797,898,240\n",
       "│    └─ModuleList: 2-13                       --                        --                        (recursive)               --\n",
       "│    │    └─PointNetSetAbstraction: 3-21      [8, 3, 1024]              [8, 128, 512]             133,760                   17,364,428,800\n",
       "│    └─ModuleList: 2-18                       --                        --                        (recursive)               --\n",
       "│    │    └─Neighbor2PointAttention: 3-22     [8, 128, 512]             [8, 128, 512]             180,736                   4,898,951,168\n",
       "│    └─ModuleList: 2-17                       --                        --                        (recursive)               --\n",
       "│    │    └─UpSampleInterpolation: 3-23       [8, 128, 1024]            [8, 128, 1024]            49,664                    335,548,416\n",
       "│    └─ModuleList: 2-18                       --                        --                        (recursive)               --\n",
       "│    │    └─Neighbor2PointAttention: 3-24     [8, 128, 1024]            [8, 128, 1024]            180,736                   9,797,898,240\n",
       "│    └─ModuleList: 2-17                       --                        --                        (recursive)               --\n",
       "│    │    └─UpSampleInterpolation: 3-25       [8, 128, 2048]            [8, 128, 2048]            49,664                    671,092,736\n",
       "│    └─ModuleList: 2-18                       --                        --                        (recursive)               --\n",
       "│    │    └─Neighbor2PointAttention: 3-26     [8, 128, 2048]            [8, 128, 2048]            180,736                   19,595,792,384\n",
       "├─Sequential: 1-3                             [8, 128, 2048]            [8, 1024, 2048]           --                        --\n",
       "│    └─Conv1d: 2-19                           [8, 128, 2048]            [8, 1024, 2048]           131,072                   2,147,483,648\n",
       "│    └─BatchNorm1d: 2-20                      [8, 1024, 2048]           [8, 1024, 2048]           2,048                     16,384\n",
       "│    └─LeakyReLU: 2-21                        [8, 1024, 2048]           [8, 1024, 2048]           --                        --\n",
       "├─Sequential: 1-4                             [8, 16, 1]                [8, 64, 1]                --                        --\n",
       "│    └─Conv1d: 2-22                           [8, 16, 1]                [8, 64, 1]                1,024                     8,192\n",
       "│    └─BatchNorm1d: 2-23                      [8, 64, 1]                [8, 64, 1]                128                       1,024\n",
       "│    └─LeakyReLU: 2-24                        [8, 64, 1]                [8, 64, 1]                --                        --\n",
       "├─Sequential: 1-5                             [8, 2240, 2048]           [8, 1024, 2048]           --                        --\n",
       "│    └─Conv1d: 2-25                           [8, 2240, 2048]           [8, 1024, 2048]           2,293,760                 37,580,963,840\n",
       "│    └─BatchNorm1d: 2-26                      [8, 1024, 2048]           [8, 1024, 2048]           2,048                     16,384\n",
       "│    └─LeakyReLU: 2-27                        [8, 1024, 2048]           [8, 1024, 2048]           --                        --\n",
       "├─Dropout: 1-6                                [8, 1024, 2048]           [8, 1024, 2048]           --                        --\n",
       "├─Sequential: 1-7                             [8, 1024, 2048]           [8, 256, 2048]            --                        --\n",
       "│    └─Conv1d: 2-28                           [8, 1024, 2048]           [8, 256, 2048]            262,144                   4,294,967,296\n",
       "│    └─BatchNorm1d: 2-29                      [8, 256, 2048]            [8, 256, 2048]            512                       4,096\n",
       "│    └─LeakyReLU: 2-30                        [8, 256, 2048]            [8, 256, 2048]            --                        --\n",
       "├─Dropout: 1-8                                [8, 256, 2048]            [8, 256, 2048]            --                        --\n",
       "├─Conv1d: 1-9                                 [8, 256, 2048]            [8, 50, 2048]             12,800                    209,715,200\n",
       "=================================================================================================================================================\n",
       "Total params: 4,794,633\n",
       "Trainable params: 4,794,633\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 176.46\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 12665.07\n",
       "Params size (MB): 19.18\n",
       "Estimated Total Size (MB): 12684.45\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import seg_model\n",
    "import subprocess\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from utils.check_config import set_config_run\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "\n",
    "subprocess.run('nvidia-smi', shell=True, text=True, stdout=None, stderr=subprocess.PIPE)\n",
    "config = OmegaConf.load('configs/default.yaml')\n",
    "cmd_config = {\n",
    "    'usr_config': 'configs/seg_boltzmannT01_bin6.yaml',\n",
    "    'datasets': 'shapenet_AnTao350M',\n",
    "    'wandb': {'name': '2024_04_16_01_26_Shapenet_Token_Std_logmean_1'},\n",
    "    'test': {'ddp': {'which_gpu': [3]}}\n",
    "}\n",
    "config = OmegaConf.merge(config, OmegaConf.create(cmd_config))\n",
    "\n",
    "dataset_config = OmegaConf.load(f'configs/datasets/{config.datasets}.yaml')\n",
    "dataset_config = OmegaConf.create({'datasets': dataset_config})\n",
    "config = OmegaConf.merge(config, dataset_config)\n",
    "\n",
    "# get test configurations\n",
    "if config.usr_config:\n",
    "    test_config = OmegaConf.load(config.usr_config)\n",
    "    config = OmegaConf.merge(config, test_config)\n",
    "\n",
    "# download artifacts\n",
    "if config.wandb.enable:\n",
    "    wandb.login(key=config.wandb.api_key)\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(f'{config.wandb.entity}/{config.wandb.project}/{config.wandb.name}:latest')\n",
    "    if config.test.suffix.enable:\n",
    "        local_path = f'./artifacts/{config.wandb.name}_{config.test.suffix.remark}'\n",
    "    else:\n",
    "        local_path = f'./artifacts/{config.wandb.name}'\n",
    "    # artifact.download(root=local_path)\n",
    "else:\n",
    "    raise ValueError('W&B is not enabled!')\n",
    "\n",
    "# overwrite the default config with previous run config\n",
    "config.mode = 'test'\n",
    "run_config = OmegaConf.load(f'{local_path}/usr_config.yaml')\n",
    "if not config.test.suffix.enable:\n",
    "    config = OmegaConf.merge(config, run_config)\n",
    "else:\n",
    "    OmegaConf.save(config, f'{local_path}/usr_config_test.yaml')\n",
    "    print(f'Overwrite the previous run config with new run config.')\n",
    "config = set_config_run(config, \"test\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "# my_model = seg_model.ShapeNetModel(config)\n",
    "# my_model = seg_model.ShapeNetModel_fps(config)\n",
    "my_model = seg_model.ShapeNetModel_fpsknn(config)\n",
    "my_model = my_model.to(device)\n",
    "my_model.eval()\n",
    "\n",
    "# x = torch.randn((8, 3, 2048)).to(device)\n",
    "# category_id = torch.randint(0, 1, (8, 16, 1)).float().to(device)\n",
    "# flops = FlopCountAnalysis(my_model, (x, category_id))\n",
    "# print(f\"FLOPs: {flops.total()}\")\n",
    "\n",
    "summary(\n",
    "    my_model,\n",
    "    input_size=((8, 3, 2048), (8, 16, 1)),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\hfu\\.netrc\n",
      "  0%|          | 0/359 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 76\u001b[0m\n\u001b[0;32m     69\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     70\u001b[0m summary(\n\u001b[0;32m     71\u001b[0m     my_model,\n\u001b[0;32m     72\u001b[0m     input_size\u001b[38;5;241m=\u001b[39m((\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2048\u001b[39m), (\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m     73\u001b[0m     col_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_params\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmult_adds\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     74\u001b[0m )\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m     77\u001b[0m my_model(x, category_id)        \n\u001b[0;32m     78\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models import seg_model\n",
    "import subprocess\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from utils.check_config import set_config_run\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "subprocess.run('nvidia-smi', shell=True, text=True, stdout=None, stderr=subprocess.PIPE)\n",
    "config = OmegaConf.load('configs/default.yaml')\n",
    "cmd_config = {\n",
    "    'usr_config': 'configs/seg_boltzmannT01_bin6.yaml',\n",
    "    'datasets': 'shapenet_AnTao350M',\n",
    "    'wandb': {'name': '2024_04_16_01_26_Shapenet_Token_Std_logmean_1'},\n",
    "    'test': {'ddp': {'which_gpu': [3]}}\n",
    "}\n",
    "config = OmegaConf.merge(config, OmegaConf.create(cmd_config))\n",
    "\n",
    "dataset_config = OmegaConf.load(f'configs/datasets/{config.datasets}.yaml')\n",
    "dataset_config = OmegaConf.create({'datasets': dataset_config})\n",
    "config = OmegaConf.merge(config, dataset_config)\n",
    "\n",
    "# get test configurations\n",
    "if config.usr_config:\n",
    "    test_config = OmegaConf.load(config.usr_config)\n",
    "    config = OmegaConf.merge(config, test_config)\n",
    "\n",
    "# download artifacts\n",
    "if config.wandb.enable:\n",
    "    wandb.login(key=config.wandb.api_key)\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(f'{config.wandb.entity}/{config.wandb.project}/{config.wandb.name}:latest')\n",
    "    if config.test.suffix.enable:\n",
    "        local_path = f'./artifacts/{config.wandb.name}_{config.test.suffix.remark}'\n",
    "    else:\n",
    "        local_path = f'./artifacts/{config.wandb.name}'\n",
    "    # artifact.download(root=local_path)\n",
    "else:\n",
    "    raise ValueError('W&B is not enabled!')\n",
    "\n",
    "# overwrite the default config with previous run config\n",
    "config.mode = 'test'\n",
    "run_config = OmegaConf.load(f'{local_path}/usr_config.yaml')\n",
    "if not config.test.suffix.enable:\n",
    "    config = OmegaConf.merge(config, run_config)\n",
    "else:\n",
    "    OmegaConf.save(config, f'{local_path}/usr_config_test.yaml')\n",
    "    print(f'Overwrite the previous run config with new run config.')\n",
    "config = set_config_run(config, \"test\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "my_model = seg_model.ShapeNetModel(config)\n",
    "my_model = my_model.to(device)\n",
    "my_model.eval()\n",
    "total_time=0\n",
    "\n",
    "for i in tqdm(range(359)):\n",
    "    with torch.no_grad():\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        x = torch.randn((8, 3, 2048)).to(device)\n",
    "        category_id = torch.randint(0, 1, (8, 16, 1)).float().to(device)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()       \n",
    "\n",
    "        my_model(x, category_id)        \n",
    "        torch.cuda.synchronize()\n",
    "        total_time += time.time() - start_time\n",
    "\n",
    "my_model = seg_model.ShapeNetModel_inference_time(config)\n",
    "my_model = my_model.to(device)\n",
    "my_model.eval()\n",
    "inference_time_ds_1 = 0\n",
    "inference_time_ds_2 = 0\n",
    "\n",
    "for i in tqdm(range(359)):\n",
    "    with torch.no_grad():\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        x = torch.randn((8, 3, 2048)).to(device)\n",
    "        category_id = torch.randint(0, 1, (8, 16, 1)).float().to(device)\n",
    "        preds, time_list = my_model(x, category_id)\n",
    "        inference_time_ds_1 += time_list[0]\n",
    "        inference_time_ds_2 += time_list[1]\n",
    "\n",
    "print(total_time )\n",
    "print(inference_time_ds_1 )\n",
    "print(inference_time_ds_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfuhaodsst\u001b[0m (\u001b[33mapes3\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\hfu\\.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   19 of 19 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "from models import seg_model\n",
    "import subprocess\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from utils.check_config import set_config_run\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "subprocess.run(\"nvidia-smi\", shell=True, text=True, stdout=None, stderr=subprocess.PIPE)\n",
    "config = OmegaConf.load(\"configs/default.yaml\")\n",
    "cmd_config = {\n",
    "    \"usr_config\": \"configs/seg_boltzmannT01_bin6.yaml\",\n",
    "    \"datasets\": \"shapenet_AnTao350M\",\n",
    "    \"wandb\": {\"name\": \"2024_04_16_01_26_Shapenet_Token_Std_logmean_1\"},\n",
    "    \"test\": {\"ddp\": {\"which_gpu\": [3]}},\n",
    "}\n",
    "config = OmegaConf.merge(config, OmegaConf.create(cmd_config))\n",
    "\n",
    "dataset_config = OmegaConf.load(f\"configs/datasets/{config.datasets}.yaml\")\n",
    "dataset_config = OmegaConf.create({\"datasets\": dataset_config})\n",
    "config = OmegaConf.merge(config, dataset_config)\n",
    "\n",
    "# get test configurations\n",
    "if config.usr_config:\n",
    "    test_config = OmegaConf.load(config.usr_config)\n",
    "    config = OmegaConf.merge(config, test_config)\n",
    "\n",
    "# download artifacts\n",
    "if config.wandb.enable:\n",
    "    wandb.login(key=config.wandb.api_key)\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(\n",
    "        f\"{config.wandb.entity}/{config.wandb.project}/{config.wandb.name}:latest\"\n",
    "    )\n",
    "    if config.test.suffix.enable:\n",
    "        local_path = f\"./artifacts/{config.wandb.name}_{config.test.suffix.remark}\"\n",
    "    else:\n",
    "        local_path = f\"./artifacts/{config.wandb.name}\"\n",
    "    artifact.download(root=local_path)\n",
    "else:\n",
    "    raise ValueError(\"W&B is not enabled!\")\n",
    "\n",
    "# overwrite the default config with previous run config\n",
    "config.mode = \"test\"\n",
    "run_config = OmegaConf.load(f\"{local_path}/usr_config.yaml\")\n",
    "if not config.test.suffix.enable:\n",
    "    config = OmegaConf.merge(config, run_config)\n",
    "else:\n",
    "    OmegaConf.save(config, f\"{local_path}/usr_config_test.yaml\")\n",
    "    print(f\"Overwrite the previous run config with new run config.\")\n",
    "config = set_config_run(config, \"test\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "my_model = seg_model.ShapeNetModel_fps(config)\n",
    "my_model = my_model.to(device)\n",
    "my_model.eval()\n",
    "total_time = 0\n",
    "for i in tqdm(range(359)):\n",
    "    with torch.no_grad():\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        x = torch.randn((8, 3, 2048)).to(device)\n",
    "        category_id = torch.randint(0, 1, (8, 16, 1)).float().to(device)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        my_model(x, category_id)\n",
    "        torch.cuda.synchronize()\n",
    "        total_time += time.time() - start_time\n",
    "\n",
    "my_model = seg_model.ShapeNetModel_inference_time_fps(config)\n",
    "my_model = my_model.to(device)\n",
    "my_model.eval()\n",
    "inference_time_ds_1 = 0\n",
    "inference_time_ds_2 = 0\n",
    "for i in tqdm(range(359)):\n",
    "    with torch.no_grad():\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        x = torch.randn((8, 3, 2048)).to(device)\n",
    "        category_id = torch.randint(0, 1, (8, 16, 1)).float().to(device)\n",
    "        preds, time_list = my_model(x, category_id)\n",
    "        inference_time_ds_1 += time_list[0]\n",
    "        inference_time_ds_2 += time_list[1]\n",
    "\n",
    "print(total_time)\n",
    "print(inference_time_ds_1)\n",
    "print(inference_time_ds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfuhaodsst\u001b[0m (\u001b[33mapes3\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\hfu\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   19 of 19 files downloaded.  \n",
      "100%|██████████| 359/359 [00:20<00:00, 17.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.82380700111389\n",
      "4.395751714706421\n",
      "2.3908379077911377\n"
     ]
    }
   ],
   "source": [
    "from models import cls_model\n",
    "import subprocess\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from utils.check_config import set_config_run\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "subprocess.run('nvidia-smi', shell=True, text=True, stdout=None, stderr=subprocess.PIPE)\n",
    "config = OmegaConf.load('configs/default.yaml')\n",
    "cmd_config = {\n",
    "    \"usr_config\": \"configs/boltzmannT0102.yaml\",\n",
    "    \"datasets\": \"modelnet_AnTao420M\",\n",
    "    \"wandb\": {\"name\": \"2024_04_28_07_27_Modelnet_Token_Std_boltzmann_T01_bin6_4\"},\n",
    "    \"test\": {\"ddp\": {\"which_gpu\": [0]}},\n",
    "}\n",
    "config = OmegaConf.merge(config, OmegaConf.create(cmd_config))\n",
    "\n",
    "dataset_config = OmegaConf.load(f'configs/datasets/{config.datasets}.yaml')\n",
    "dataset_config = OmegaConf.create({'datasets': dataset_config})\n",
    "config = OmegaConf.merge(config, dataset_config)\n",
    "\n",
    "# get test configurations\n",
    "if config.usr_config:\n",
    "    test_config = OmegaConf.load(config.usr_config)\n",
    "    config = OmegaConf.merge(config, test_config)\n",
    "\n",
    "# download artifacts\n",
    "if config.wandb.enable:\n",
    "    wandb.login(key=config.wandb.api_key)\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(\n",
    "        f\"{config.wandb.entity}/{config.wandb.project}/{config.wandb.name}:latest\"\n",
    "    )\n",
    "    if config.test.suffix.enable:\n",
    "        local_path = f\"./artifacts/{config.wandb.name}_{config.test.suffix.remark}\"\n",
    "    else:\n",
    "        local_path = f\"./artifacts/{config.wandb.name}\"\n",
    "    artifact.download(root=local_path)\n",
    "else:\n",
    "    raise ValueError(\"W&B is not enabled!\")\n",
    "\n",
    "# overwrite the default config with previous run config\n",
    "config.mode = \"test\"\n",
    "run_config = OmegaConf.load(f\"{local_path}/usr_config.yaml\")\n",
    "if not config.test.suffix.enable:\n",
    "    config = OmegaConf.merge(config, run_config)\n",
    "else:\n",
    "    OmegaConf.save(config, f\"{local_path}/usr_config_test.yaml\")\n",
    "    print(f\"Overwrite the previous run config with new run config.\")\n",
    "config = set_config_run(config, \"test\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "my_model = cls_model.ModelNetModel(config)\n",
    "my_model.eval()\n",
    "my_model = my_model.to(device)\n",
    "\n",
    "total_time = 0\n",
    "for i in tqdm(range(359)):\n",
    "    with torch.no_grad():  # x.shape == (B, 3, N)\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        samples = torch.randn((8, 3, 2048)).to(device)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        my_model(samples)\n",
    "        torch.cuda.synchronize()\n",
    "        total_time += time.time() - start_time\n",
    "print(total_time)\n",
    "\n",
    "my_model = cls_model.ModelNetModel_inference_time(config)\n",
    "my_model = my_model.to(device)\n",
    "my_model.eval()\n",
    "inference_time_ds_1 = 0\n",
    "inference_time_ds_2 = 0\n",
    "\n",
    "for i in tqdm(range(359)):\n",
    "    with torch.no_grad():\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        x = torch.randn((8, 3, 2048)).to(device)\n",
    "        preds, time_list = my_model(samples)\n",
    "        inference_time_ds_1 += time_list[0]\n",
    "        inference_time_ds_2 += time_list[1]\n",
    "\n",
    "\n",
    "print(inference_time_ds_1)\n",
    "print(inference_time_ds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\hfu\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   19 of 19 files downloaded.  \n",
      "100%|██████████| 359/359 [03:49<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229.27262926101685\n",
      "143.62903594970703\n",
      "68.53185868263245\n"
     ]
    }
   ],
   "source": [
    "from models import cls_model\n",
    "import subprocess\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from utils.check_config import set_config_run\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "subprocess.run(\"nvidia-smi\", shell=True, text=True, stdout=None, stderr=subprocess.PIPE)\n",
    "config = OmegaConf.load(\"configs/default.yaml\")\n",
    "cmd_config = {\n",
    "    \"usr_config\": \"configs/boltzmannT0102.yaml\",\n",
    "    \"datasets\": \"modelnet_AnTao420M\",\n",
    "    \"wandb\": {\"name\": \"2024_04_28_07_27_Modelnet_Token_Std_boltzmann_T01_bin6_4\"},\n",
    "    \"test\": {\"ddp\": {\"which_gpu\": [0]}},\n",
    "}\n",
    "config = OmegaConf.merge(config, OmegaConf.create(cmd_config))\n",
    "\n",
    "dataset_config = OmegaConf.load(f\"configs/datasets/{config.datasets}.yaml\")\n",
    "dataset_config = OmegaConf.create({\"datasets\": dataset_config})\n",
    "config = OmegaConf.merge(config, dataset_config)\n",
    "\n",
    "# get test configurations\n",
    "if config.usr_config:\n",
    "    test_config = OmegaConf.load(config.usr_config)\n",
    "    config = OmegaConf.merge(config, test_config)\n",
    "\n",
    "# download artifacts\n",
    "if config.wandb.enable:\n",
    "    wandb.login(key=config.wandb.api_key)\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(\n",
    "        f\"{config.wandb.entity}/{config.wandb.project}/{config.wandb.name}:latest\"\n",
    "    )\n",
    "    if config.test.suffix.enable:\n",
    "        local_path = f\"./artifacts/{config.wandb.name}_{config.test.suffix.remark}\"\n",
    "    else:\n",
    "        local_path = f\"./artifacts/{config.wandb.name}\"\n",
    "    artifact.download(root=local_path)\n",
    "else:\n",
    "    raise ValueError(\"W&B is not enabled!\")\n",
    "\n",
    "# overwrite the default config with previous run config\n",
    "config.mode = \"test\"\n",
    "run_config = OmegaConf.load(f\"{local_path}/usr_config.yaml\")\n",
    "if not config.test.suffix.enable:\n",
    "    config = OmegaConf.merge(config, run_config)\n",
    "else:\n",
    "    OmegaConf.save(config, f\"{local_path}/usr_config_test.yaml\")\n",
    "    print(f\"Overwrite the previous run config with new run config.\")\n",
    "config = set_config_run(config, \"test\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "my_model = cls_model.ModelNetModel_fps(config)\n",
    "my_model.eval()\n",
    "my_model = my_model.to(device)\n",
    "\n",
    "total_time = 0\n",
    "for i in tqdm(range(359)):\n",
    "    with torch.no_grad():  # x.shape == (B, 3, N)\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        samples = torch.randn((8, 3, 2048)).to(device)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        my_model(samples)\n",
    "        torch.cuda.synchronize()\n",
    "        total_time += time.time() - start_time\n",
    "print(total_time)\n",
    "\n",
    "my_model = cls_model.ModelNetModel_fps_inference_time(config)\n",
    "my_model = my_model.to(device)\n",
    "my_model.eval()\n",
    "inference_time_ds_1 = 0\n",
    "inference_time_ds_2 = 0\n",
    "\n",
    "for i in tqdm(range(359)):\n",
    "    with torch.no_grad():\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        x = torch.randn((8, 3, 2048)).to(device)\n",
    "        preds, time_list = my_model(samples)\n",
    "        inference_time_ds_1 += time_list[0]\n",
    "        inference_time_ds_2 += time_list[1]\n",
    "\n",
    "\n",
    "print(inference_time_ds_1)\n",
    "print(inference_time_ds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 11\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnvidia-smi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m config \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs/default.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m cmd_config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musr_config\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs/boltzmannT0102.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelnet_AnTao420M\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024_04_28_07_27_Modelnet_Token_Std_boltzmann_T01_bin6_4\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mddp\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich_gpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0\u001b[39m]}},\n\u001b[0;32m     18\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\hfu\\AppData\\Local\\anaconda3\\envs\\apesv3\\lib\\subprocess.py:507\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 507\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    509\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[1;32mc:\\Users\\hfu\\AppData\\Local\\anaconda3\\envs\\apesv3\\lib\\subprocess.py:1124\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr:\n\u001b[1;32m-> 1124\u001b[0m     stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait()\n",
      "File \u001b[1;32mc:\\Users\\hfu\\AppData\\Local\\anaconda3\\envs\\apesv3\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models import cls_model\n",
    "import subprocess\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from utils.check_config import set_config_run\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "subprocess.run(\"nvidia-smi\", shell=True, text=True, stdout=None, stderr=subprocess.PIPE)\n",
    "config = OmegaConf.load(\"configs/default.yaml\")\n",
    "cmd_config = {\n",
    "    \"usr_config\": \"configs/boltzmannT0102.yaml\",\n",
    "    \"datasets\": \"modelnet_AnTao420M\",\n",
    "    \"wandb\": {\"name\": \"2024_04_28_07_27_Modelnet_Token_Std_boltzmann_T01_bin6_4\"},\n",
    "    \"test\": {\"ddp\": {\"which_gpu\": [0]}},\n",
    "}\n",
    "config = OmegaConf.merge(config, OmegaConf.create(cmd_config))\n",
    "\n",
    "dataset_config = OmegaConf.load(f\"configs/datasets/{config.datasets}.yaml\")\n",
    "dataset_config = OmegaConf.create({\"datasets\": dataset_config})\n",
    "config = OmegaConf.merge(config, dataset_config)\n",
    "\n",
    "# get test configurations\n",
    "if config.usr_config:\n",
    "    test_config = OmegaConf.load(config.usr_config)\n",
    "    config = OmegaConf.merge(config, test_config)\n",
    "\n",
    "# download artifacts\n",
    "if config.wandb.enable:\n",
    "    wandb.login(key=config.wandb.api_key)\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(\n",
    "        f\"{config.wandb.entity}/{config.wandb.project}/{config.wandb.name}:latest\"\n",
    "    )\n",
    "    if config.test.suffix.enable:\n",
    "        local_path = f\"./artifacts/{config.wandb.name}_{config.test.suffix.remark}\"\n",
    "    else:\n",
    "        local_path = f\"./artifacts/{config.wandb.name}\"\n",
    "    # artifact.download(root=local_path)\n",
    "else:\n",
    "    raise ValueError(\"W&B is not enabled!\")\n",
    "\n",
    "# overwrite the default config with previous run config\n",
    "config.mode = \"test\"\n",
    "run_config = OmegaConf.load(f\"{local_path}/usr_config.yaml\")\n",
    "if not config.test.suffix.enable:\n",
    "    config = OmegaConf.merge(config, run_config)\n",
    "else:\n",
    "    OmegaConf.save(config, f\"{local_path}/usr_config_test.yaml\")\n",
    "    print(f\"Overwrite the previous run config with new run config.\")\n",
    "config = set_config_run(config, \"test\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "my_model = cls_model.ModelNetModel_fpsknn(config)\n",
    "my_model.eval()\n",
    "my_model = my_model.to(device)\n",
    "\n",
    "total_time = 0\n",
    "for i in tqdm(range(359)):\n",
    "    with torch.no_grad():  # x.shape == (B, 3, N)\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        samples = torch.randn((8, 3, 2048)).to(device)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        my_model(samples)\n",
    "        torch.cuda.synchronize()\n",
    "        total_time += time.time() - start_time\n",
    "print(total_time)\n",
    "\n",
    "my_model = cls_model.ModelNetModel_fpsknn_inference_time(config)\n",
    "my_model = my_model.to(device)\n",
    "my_model.eval()\n",
    "inference_time_ds_1 = 0\n",
    "inference_time_ds_2 = 0\n",
    "\n",
    "for i in tqdm(range(359)):\n",
    "    with torch.no_grad():\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        x = torch.randn((8, 3, 2048)).to(device)\n",
    "        preds, time_list = my_model(samples)\n",
    "        inference_time_ds_1 += time_list[0]\n",
    "        inference_time_ds_2 += time_list[1]\n",
    "\n",
    "\n",
    "print(inference_time_ds_1)\n",
    "print(inference_time_ds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfuhaodsst\u001b[0m (\u001b[33mapes3\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\hfu\\.netrc\n"
     ]
    }
   ],
   "source": [
    "from models import seg_model\n",
    "import subprocess\n",
    "from omegaconf import OmegaConf\n",
    "import wandb\n",
    "from utils.check_config import set_config_run\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "subprocess.run(\"nvidia-smi\", shell=True, text=True, stdout=None, stderr=subprocess.PIPE)\n",
    "config = OmegaConf.load(\"configs/default.yaml\")\n",
    "cmd_config = {\n",
    "    \"usr_config\": \"configs/seg_boltzmannT01_bin6.yaml\",\n",
    "    \"datasets\": \"shapenet_AnTao350M\",\n",
    "    \"wandb\": {\"name\": \"2024_04_16_01_26_Shapenet_Token_Std_logmean_1\"},\n",
    "    \"test\": {\"ddp\": {\"which_gpu\": [3]}},\n",
    "}\n",
    "config = OmegaConf.merge(config, OmegaConf.create(cmd_config))\n",
    "\n",
    "dataset_config = OmegaConf.load(f\"configs/datasets/{config.datasets}.yaml\")\n",
    "dataset_config = OmegaConf.create({\"datasets\": dataset_config})\n",
    "config = OmegaConf.merge(config, dataset_config)\n",
    "\n",
    "# get test configurations\n",
    "if config.usr_config:\n",
    "    test_config = OmegaConf.load(config.usr_config)\n",
    "    config = OmegaConf.merge(config, test_config)\n",
    "\n",
    "# download artifacts\n",
    "if config.wandb.enable:\n",
    "    wandb.login(key=config.wandb.api_key)\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(\n",
    "        f\"{config.wandb.entity}/{config.wandb.project}/{config.wandb.name}:latest\"\n",
    "    )\n",
    "    if config.test.suffix.enable:\n",
    "        local_path = f\"./artifacts/{config.wandb.name}_{config.test.suffix.remark}\"\n",
    "    else:\n",
    "        local_path = f\"./artifacts/{config.wandb.name}\"\n",
    "    # artifact.download(root=local_path)\n",
    "else:\n",
    "    raise ValueError(\"W&B is not enabled!\")\n",
    "\n",
    "# overwrite the default config with previous run config\n",
    "config.mode = \"test\"\n",
    "run_config = OmegaConf.load(f\"{local_path}/usr_config.yaml\")\n",
    "if not config.test.suffix.enable:\n",
    "    config = OmegaConf.merge(config, run_config)\n",
    "else:\n",
    "    OmegaConf.save(config, f\"{local_path}/usr_config_test.yaml\")\n",
    "    print(f\"Overwrite the previous run config with new run config.\")\n",
    "config = set_config_run(config, \"test\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "my_model = seg_model.ShapeNetModel_fpsknn(config)\n",
    "my_model = my_model.to(device)\n",
    "my_model.eval()\n",
    "total_time = 0\n",
    "for i in tqdm(range(359)):  # 359\n",
    "    with torch.no_grad():\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        x = torch.randn((8, 3, 2048)).to(device)\n",
    "        category_id = torch.randint(0, 1, (8, 16, 1)).float().to(device)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        my_model(x, category_id)\n",
    "        torch.cuda.synchronize()\n",
    "        total_time += time.time() - start_time\n",
    "\n",
    "my_model = seg_model.ShapeNetModel_inference_time_fpsknn(config)\n",
    "my_model = my_model.to(device)\n",
    "my_model.eval()\n",
    "inference_time_ds_1 = 0\n",
    "inference_time_ds_2 = 0\n",
    "for i in tqdm(range(359)):\n",
    "    with torch.no_grad():\n",
    "        # x.shape == (B, 3, N)  category_id.shape == (B, 16, 1)\n",
    "        x = torch.randn((8, 3, 2048)).to(device)\n",
    "        category_id = torch.randint(0, 1, (8, 16, 1)).float().to(device)\n",
    "        preds, time_list = my_model(x, category_id)\n",
    "        inference_time_ds_1 += time_list[0]\n",
    "        inference_time_ds_2 += time_list[1]\n",
    "\n",
    "print(total_time)\n",
    "print(inference_time_ds_1)\n",
    "print(inference_time_ds_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apesv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
